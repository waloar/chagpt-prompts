{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waloar/chagpt-prompts/blob/main/LangChain_PydanticOutputParser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lang Chain PydanticOutputParser\n",
        "or *Conversational Directed Graph Traversal with Lang Chain*\n",
        "\n",
        "This notebook explores the use of Lang Chain for a specific business usecase for LLMs. Feel free to read the article [here](https://medium.com/@danielwarfield1/conversations-as-directed-graphs-with-lang-chain-46d70e1a846c).\n"
      ],
      "metadata": {
        "id": "otGsbRTHogJ5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgzZooHUoIQw",
        "outputId": "b842d9ec-7899-4b72-8aa6-1dcb00384767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#copying from google drive to local\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open (\"/content/drive/MyDrive/chatgpt-test/apikey.txt\", \"r\") as myfile:\n",
        "    OPENAI_API_TOKEN = myfile.read()\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN\n",
        "print('API Key Loaded!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ek6q7CEzwfqd",
        "outputId": "fabfd07a-e78c-4d6f-9741-86386fb90360"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key Loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOVlPThsspnt",
        "outputId": "4d11e5c3-ed6e-40f2-80dc-8cda8b74e77f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.314-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.21)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.43 (from langchain)\n",
            "  Downloading langsmith-0.0.43-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.314 langsmith-0.0.43 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2hxlqKgGzte",
        "outputId": "3bedbf70-2a2e-4817-b3e5-ee72e4754907"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m752.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Libraries"
      ],
      "metadata": {
        "id": "8yK5YQjLBCJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.pydantic_v1 import BaseModel, Field, validator"
      ],
      "metadata": {
        "id": "tmayI2-QBVHP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definind Utilities"
      ],
      "metadata": {
        "id": "tGrInKzhSXSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Defining utility functions for constructing a readable exchange\n",
        "\"\"\"\n",
        "\n",
        "def system_output(output):\n",
        "    \"\"\"Function for printing out to the user\n",
        "    \"\"\"\n",
        "    print('======= Bot =======')\n",
        "    print(output)\n",
        "\n",
        "def user_input():\n",
        "    \"\"\"Function for getting user input\n",
        "    \"\"\"\n",
        "    print('======= Human Input =======')\n",
        "    return input('input: ')\n",
        "\n",
        "def parsing_info(output):\n",
        "    \"\"\"Function for printing out key info\n",
        "    \"\"\"\n",
        "    print(f'*Info* {output}')\n"
      ],
      "metadata": {
        "id": "gct8xbgZSW3I"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining an Edge"
      ],
      "metadata": {
        "id": "Dc7XI7NlU71j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "class Edge:\n",
        "\n",
        "    \"\"\"Edge\n",
        "    at it's highest level, an edge checks if an input is good, then parses\n",
        "    data out of that input if it is good\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, condition, parse_prompt, parse_class, llm, max_retrys=3, out_node=None):\n",
        "        \"\"\"\n",
        "        condition (str): a True/False question about the input\n",
        "        parse_query (str): what the parser whould be extracting\n",
        "        parse_class (Pydantic BaseModel): the structure of the parse\n",
        "        llm (LangChain LLM): the large language model being used\n",
        "        \"\"\"\n",
        "        self.condition = condition\n",
        "        self.parse_prompt = parse_prompt\n",
        "        self.parse_class = parse_class\n",
        "        self.llm = llm\n",
        "\n",
        "        #how many times the edge has failed, for any reason, for deciding to skip\n",
        "        #when successful this resets to 0 for posterity.\n",
        "        self.num_fails = 0\n",
        "\n",
        "        #how many retrys are acceptable\n",
        "        self.max_retrys = max_retrys\n",
        "\n",
        "        #the node the edge directs towards\n",
        "        self.out_node = out_node\n",
        "\n",
        "    def check(self, _input):\n",
        "        \"\"\"ask the llm if the input satisfies the condition\n",
        "        \"\"\"\n",
        "        validation_query = f'following the output schema, does the input satisfy the condition?\\ninput:{_input}\\ncondition:{self.condition}'\n",
        "        class Validation(BaseModel):\n",
        "            is_valid: bool = Field(description=\"if the condition is satisfied\")\n",
        "        parser = PydanticOutputParser(pydantic_object=Validation)\n",
        "        _input = f\"Answer the user query.\\n{parser.get_format_instructions()}\\n{validation_query}\\n\"\n",
        "        return parser.parse(self.llm(_input)).is_valid\n",
        "\n",
        "    def parse(self, _input):\n",
        "        \"\"\"ask the llm to parse the parse_class, based on the parse_prompt, from the input\n",
        "        \"\"\"\n",
        "        parse_query = f'{self.parse_prompt}:\\n\\n\"{_input}\"'\n",
        "        parser = PydanticOutputParser(pydantic_object=self.parse_class)\n",
        "        _input = f\"Answer the user query.\\n{parser.get_format_instructions()}\\n{parse_query}\\n\"\n",
        "        return parser.parse(self.llm(_input))\n",
        "\n",
        "\n",
        "    def execute(self, _input):\n",
        "        \"\"\"Executes the entire edge\n",
        "        returns a dictionary:\n",
        "        {\n",
        "            continue: bool,       weather or not should continue to next\n",
        "            result: parse_class,  the parsed result, if applicable\n",
        "            num_fails: int        the number of failed attempts\n",
        "            continue_to: Node     the Node the edge continues to\n",
        "        }\n",
        "        \"\"\"\n",
        "\n",
        "        #input did't make it past the input condition for the edge\n",
        "        if not self.check(_input):\n",
        "            self.num_fails += 1\n",
        "            if self.num_fails >= self.max_retrys:\n",
        "                return {'continue': True, 'result': None, 'num_fails': self.num_fails, 'continue_to': self.out_node}\n",
        "            return {'continue': False, 'result': None, 'num_fails': self.num_fails, 'continue_to': self.out_node}\n",
        "\n",
        "        try:\n",
        "            #attempting to parse\n",
        "            self.num_fails = 0\n",
        "            return {'continue': True, 'result': self.parse(_input), 'num_fails': self.num_fails, 'continue_to': self.out_node}\n",
        "        except:\n",
        "            #there was some error in parsing.\n",
        "            #note, using the retry or correction parser here might be a good idea\n",
        "            self.num_fails += 1\n",
        "            if self.num_fails >= self.max_retrys:\n",
        "                return {'continue': True, 'result': None, 'num_fails': self.num_fails, 'continue_to': self.out_node}\n",
        "            return {'continue': False, 'result': None, 'num_fails': self.num_fails, 'continue_to': self.out_node}\n",
        "\n",
        "\n",
        "\"\"\"Running a few unit tests\n",
        "\"\"\"\n",
        "if True:\n",
        "\n",
        "    #defining the model used on the edge\n",
        "    model_name = \"text-davinci-003\"\n",
        "    temperature = 0.0\n",
        "    model = OpenAI(model_name=model_name, temperature=temperature)\n",
        "\n",
        "    #defining the desired output format, a list of fruits\n",
        "    class sampleOutputTemplate(BaseModel):\n",
        "        output: List[str] = Field(description=\"a list of only fruits\")\n",
        "\n",
        "    #defining the query for the condition, and parse prompt\n",
        "    condition = \"Does the input contain fruits?\"\n",
        "    parse_prompt = \"extract only the fruits from the following text. Do not extract any food items besides pure fruits.\"\n",
        "\n",
        "    #defining the edge\n",
        "    testEdge = Edge(condition = condition,\n",
        "                    parse_prompt = parse_prompt,\n",
        "                    parse_class = sampleOutputTemplate,\n",
        "                    llm = model)\n",
        "\n",
        "    #a sample input from the user\n",
        "    sample_input = \"my favorite deserts are chocolate covered strawberries, oreos, bannana splits, and cake.\"\n",
        "\n",
        "    print('===== testing the condition functionality =====')\n",
        "    print(f'weather or not the input \\n\"{sample_input}\"\\nsatisfies the condition\\n\"{condition}\"')\n",
        "    print('result: {}'.format(testEdge.check(sample_input)))\n",
        "\n",
        "    print('===== parse results =====')\n",
        "    print(testEdge.parse(sample_input).output)\n",
        "\n",
        "    print('===== Testing full edge execution =====')\n",
        "    print(testEdge.execute(sample_input))\n",
        "\n",
        "    print('===== Testing a few failed executions =====')\n",
        "    print(testEdge.execute('Mr.Rodgers was the closest thing to a perfect person yet attained'))\n",
        "    print(testEdge.execute('Without a doubt, The Chessapeak is the greater of the two bays'))\n",
        "    print(testEdge.execute('max retrys is 3 by default, so giving up'))\n",
        "\n",
        "    print('===== Testing a successful parse =====')\n",
        "    print(testEdge.execute('My favorite tart contains a cinnamon grahm-cracker crust with\\\n",
        "                            Vanilla custard, peach preserve, strawberries, and blueberries'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYNxaaMmU7W7",
        "outputId": "590d3a73-a9b9-4c9c-a0ef-f8877ae84c11"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== testing the condition functionality =====\n",
            "weather or not the input \n",
            "\"my favorite deserts are chocolate covered strawberries, oreos, bannana splits, and cake.\"\n",
            "satisfies the condition\n",
            "\"Does the input contain fruits?\"\n",
            "result: True\n",
            "===== parse results =====\n",
            "['strawberries', 'bannana']\n",
            "===== Testing full edge execution =====\n",
            "{'continue': True, 'result': sampleOutputTemplate(output=['strawberries', 'bannana']), 'num_fails': 0, 'continue_to': None}\n",
            "===== Testing a few failed executions =====\n",
            "{'continue': False, 'result': None, 'num_fails': 1, 'continue_to': None}\n",
            "{'continue': False, 'result': None, 'num_fails': 2, 'continue_to': None}\n",
            "{'continue': True, 'result': None, 'num_fails': 3, 'continue_to': None}\n",
            "===== Testing a successful parse =====\n",
            "{'continue': True, 'result': sampleOutputTemplate(output=['peach', 'strawberries', 'blueberries']), 'num_fails': 0, 'continue_to': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining a Node"
      ],
      "metadata": {
        "id": "BCwp56OsSM8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "\n",
        "    \"\"\"Node\n",
        "    at it's highest level, a node asks a user for some input, and trys\n",
        "    that input on all edges. It also manages and executes all\n",
        "    the edges it contains\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, prompt, retry_prompt):\n",
        "        \"\"\"\n",
        "        prompt (str): what to ask the user\n",
        "        retry_prompt (str): what to ask the user if all edges fail\n",
        "        parse_class (Pydantic BaseModel): the structure of the parse\n",
        "        llm (LangChain LLM): the large language model being used\n",
        "        \"\"\"\n",
        "\n",
        "        self.prompt = prompt\n",
        "        self.retry_prompt = retry_prompt\n",
        "        self.edges = []\n",
        "\n",
        "    def run_to_continue(self, _input):\n",
        "        \"\"\"Run all edges until one continues\n",
        "        returns the result of the continuing edge, or None\n",
        "        \"\"\"\n",
        "        for edge in self.edges:\n",
        "            res = edge.execute(_input)\n",
        "            if res['continue']: return res\n",
        "        return None\n",
        "\n",
        "    def execute(self):\n",
        "        \"\"\"Handles the current conversational state\n",
        "        prompots the user, tries again, runs edges, etc.\n",
        "        returns the result from an adge\n",
        "        \"\"\"\n",
        "\n",
        "        #initial prompt for the conversational state\n",
        "        system_output(self.prompt)\n",
        "\n",
        "        while True:\n",
        "            #getting users input\n",
        "            _input = user_input()\n",
        "\n",
        "            #running through edges\n",
        "            res = self.run_to_continue(_input)\n",
        "\n",
        "            if res is not None:\n",
        "                #parse successful\n",
        "                parsing_info(f'parse results: {res}')\n",
        "                return res\n",
        "\n",
        "            #unsuccessful, prompting retry\n",
        "            system_output(self.retry_prompt)\n",
        "\n",
        "\n",
        "\"\"\"Testing\n",
        "asking the user for an email or phone number, and attemtpting to parse either one\n",
        "\"\"\"\n",
        "if True:\n",
        "\n",
        "    #defining the model used in this test\n",
        "    model_name = \"text-davinci-003\"\n",
        "    temperature = 0.0\n",
        "    model = OpenAI(model_name=model_name, temperature=temperature)\n",
        "\n",
        "    #Defining 2 edges from the node\n",
        "    class sampleOutputTemplate(BaseModel):\n",
        "        output: str = Field(description=\"contact information\")\n",
        "    condition1 = \"Does the input contain a full and valid email?\"\n",
        "    parse_prompt1 = \"extract the email from the following text.\"\n",
        "    edge1 = Edge(condition1, parse_prompt1, sampleOutputTemplate, model)\n",
        "    condition2 = \"Does the input contain a phone number?\"\n",
        "    parse_prompt2 = \"extract the phone number from the following text.\"\n",
        "    edge2 = Edge(condition2, parse_prompt2, sampleOutputTemplate, model)\n",
        "\n",
        "    #Defining A Node\n",
        "    test_node = Node(prompt = \"Please input your full email address or phone number\",\n",
        "                     retry_prompt = \"I'm sorry, I didn't understand your response.\\nPlease provide a full email address or phone number(in the format xxx-xxx-xxxx)\")\n",
        "\n",
        "    #Defining Connections\n",
        "    test_node.edges = [edge1, edge2]\n",
        "\n",
        "    #running node. This handles all i/o and the logic to re-ask on failure.\n",
        "    res = test_node.execute()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2luww_DmSMsx",
        "outputId": "0bd1cfad-2b44-4971-d804-958f53f7638a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Bot =======\n",
            "Please input your full email address or phone number\n",
            "======= Human Input =======\n",
            "input: roperto@hotmail.com\n",
            "*Info* parse results: {'continue': True, 'result': sampleOutputTemplate(output='roperto@hotmail.com'), 'num_fails': 0, 'continue_to': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UImAU6UjtEVE",
        "outputId": "d1c299ba-20bd-443d-b8e0-ac194f577e9c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'continue': True,\n",
              " 'result': sampleOutputTemplate(output='roperto@hotmail.com'),\n",
              " 'num_fails': 0,\n",
              " 'continue_to': None}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implimenting Conversation"
      ],
      "metadata": {
        "id": "HCbRy4-AjhPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Implimenting the conversation as a directed graph\n",
        "\"\"\"\n",
        "\n",
        "# Defining Nodes\n",
        "name_node = Node(\"Hello! My name's Dana and I'll be getting you started on your renting journey. I'll be asking you a few questions, and then forwarding you to one of our excellent agents to help you find a place you'd love to call home.\\n\\nFirst, can you please provide your name?\", \"I'm sorry, I don't understand, can you provide just your name?\")\n",
        "contact_node = Node(\"do you have a phone number or email we can use to contact you?\", \"I'm sorry, I didn't understand that. Can you please provide a valid email or phone number?\")\n",
        "budget_node = Node(\"What is your monthly budget for rent?\", \"I'm sorry, I don't understand the rent you provided. Try providing your rent in a format like '$1,300'\")\n",
        "avail_node = Node(\"Great, When is your soonest availability?\", \"I'm sorry, one more time, can you please provide a date you're willing to meet?\")\n",
        "\n",
        "#Defining Data Structures for Parsing\n",
        "class nameTemplate(BaseModel): output: str = Field(description=\"a persons name\")\n",
        "class phoneTemplate(BaseModel): output: str = Field(description=\"phone number\")\n",
        "class emailTemplate(BaseModel): output: str = Field(description=\"email address\")\n",
        "class budgetTemplate(BaseModel): output: float = Field(description=\"budget\")\n",
        "class dateTemplate(BaseModel): output: str = Field(description=\"date\")\n",
        "\n",
        "#defining the model\n",
        "model_name = \"text-davinci-003\"\n",
        "temperature = 0.0\n",
        "model = OpenAI(model_name=model_name, temperature=temperature)\n",
        "\n",
        "#Defining Edges\n",
        "name_edge = Edge(\"Does the input contain a persons name?\", \" Extract the persons name from the following text.\", nameTemplate, model)\n",
        "contact_phone_edge = Edge(\"does the input contain a valid phone number?\", \"extract the phone number in the format xxx-xxx-xxxx\", phoneTemplate, model)\n",
        "contact_email_edge = Edge(\"does the input contain a valid email?\", \"extract the email from the following text\", emailTemplate, model)\n",
        "budget_edge = Edge(\"Does the input contain a number in the thousands?\", \"Extract the number from the following text from the following text. Remove any symbols and multiply a number followed by the letter 'k' to thousands.\", budgetTemplate, model)\n",
        "avail_edge = Edge(\"does the input contain a date or day? dates or relative terms like 'tommorrow' or 'in 2 days'.\", \"extract the day discussed in the following text as a date in mm/dd/yyyy format. Today is September 23rd 2023.\", dateTemplate, model)\n",
        "\n",
        "#Defining Node Connections\n",
        "name_node.edges = [name_edge]\n",
        "contact_node.edges = [contact_phone_edge, contact_email_edge]\n",
        "budget_node.edges = [budget_edge]\n",
        "avail_node.edges = [avail_edge]\n",
        "\n",
        "#defining edge connections\n",
        "name_edge.out_node = contact_node\n",
        "contact_phone_edge.out_node = budget_node\n",
        "contact_email_edge.out_node = budget_node\n",
        "budget_edge.out_node = avail_node\n",
        "\n",
        "#running the graph\n",
        "current_node = name_node\n",
        "while current_node is not None:\n",
        "    res = current_node.execute()\n",
        "    if res['continue']:\n",
        "        current_node = res['continue_to']\n",
        "        print(current_node)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "ififa6CqjiwC",
        "outputId": "566a28ea-15a9-4491-d12c-35f7e96ea9d9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Bot =======\n",
            "Hello! My name's Dana and I'll be getting you started on your renting journey. I'll be asking you a few questions, and then forwarding you to one of our excellent agents to help you find a place you'd love to call home.\n",
            "\n",
            "First, can you please provide your name?\n",
            "======= Human Input =======\n",
            "input: walter\n",
            "======= Bot =======\n",
            "I'm sorry, I don't understand, can you provide just your name?\n",
            "======= Human Input =======\n",
            "input: roberto\n",
            "*Info* parse results: {'continue': True, 'result': nameTemplate(output='roberto'), 'num_fails': 0, 'continue_to': <__main__.Node object at 0x7d8541796170>}\n",
            "<__main__.Node object at 0x7d8541796170>\n",
            "======= Bot =======\n",
            "do you have a phone number or email we can use to contact you?\n",
            "======= Human Input =======\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-1b46fbef7b68>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mcurrent_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mcurrent_node\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'continue'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mcurrent_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'continue_to'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-e95de3b78734>\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m#getting users input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0m_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m#running through edges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-f7f19a9a5fb4>\u001b[0m in \u001b[0;36muser_input\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \"\"\"\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'======= Human Input ======='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparsing_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Implimenting the conversation as a directed graph\n",
        "\"\"\"\n",
        "\n",
        "# Defining Nodes\n",
        "intencion_node = Node(\"Hola! soy Dana tu asistente. Estoy para asistirte. En el camino seguramente te hare algunas pregutnas. En que puedo ayudarte ? \", \"Disculpa, no te entiendo. Podrias repetir ?\")\n",
        "name_node = Node(\"Hello! My name's Dana and I'll be getting you started on your renting journey. I'll be asking you a few questions, and then forwarding you to one of our excellent agents to help you find a place you'd love to call home.\\n\\nFirst, can you please provide your name?\", \"I'm sorry, I don't understand, can you provide just your name?\")\n",
        "dni_node = Node(\"Podrias decirme tu DNI ?\", \"Disculpa, no entiendo. Necesitamos tu numero de documento para poder continuar. Podrias facilitarlo ?\")\n",
        "contact_node = Node(\"do you have a phone number or email we can use to contact you?\", \"I'm sorry, I didn't understand that. Can you please provide a valid email or phone number?\")\n",
        "budget_node = Node(\"What is your monthly budget for rent?\", \"I'm sorry, I don't understand the rent you provided. Try providing your rent in a format like '$1,300'\")\n",
        "avail_node = Node(\"Great, When is your soonest availability?\", \"I'm sorry, one more time, can you please provide a date you're willing to meet?\")\n",
        "\n",
        "#Defining Data Structures for Parsing\n",
        "class saludoTemplate(BaseModel): output: str = Field(description=\"un saludo\")\n",
        "class nameTemplate(BaseModel): output: str = Field(description=\"a persons name\")\n",
        "class dniTemplate(BaseModel): output: str = Field(description=\"documento nacional de identidad de entre 7 u 8 numeros\")\n",
        "class phoneTemplate(BaseModel): output: str = Field(description=\"phone number\")\n",
        "class emailTemplate(BaseModel): output: str = Field(description=\"email address\")\n",
        "class budgetTemplate(BaseModel): output: float = Field(description=\"budget\")\n",
        "class dateTemplate(BaseModel): output: str = Field(description=\"date\")\n",
        "\n",
        "#defining the model\n",
        "model_name = \"text-davinci-003\"\n",
        "temperature = 0.0\n",
        "model = OpenAI(model_name=model_name, temperature=temperature)\n",
        "\n",
        "#Defining Edges\n",
        "intencion_edge = Edge(\"El input contiene un saludo o frase? \", \"Resume la frase o saludo en un maximo de dos palabras.\", saludoTemplate, model)\n",
        "name_edge = Edge(\"Does the input contain a persons name?\", \" Extract the persons name from the following text.\", nameTemplate, model)\n",
        "dni_edge = Edge(\"El input contiene un numero de entre 7 u 8 digitos ?\", \"Language: Spanish. El documento  es un numero de entre 7 u 8 digitos.  Ejemplo: 92837232 o 0928272 o 23.659.494. Extrae el DNI con el siguiente formato XX.XXX.XXX.\", dniTemplate, model)\n",
        "contact_phone_edge = Edge(\"does the input contain a valid phone number?\", \"extract the phone number in the format xxx-xxx-xxxx\", phoneTemplate, model)\n",
        "contact_email_edge = Edge(\"does the input contain a valid email?\", \"extract the email from the following text\", emailTemplate, model)\n",
        "budget_edge = Edge(\"Does the input contain a number in the thousands?\", \"Extract the number from the following text from the following text. Remove any symbols and multiply a number followed by the letter 'k' to thousands.\", budgetTemplate, model)\n",
        "avail_edge = Edge(\"does the input contain a date or day? dates or relative terms like 'tommorrow' or 'in 2 days'.\", \"extract the day discussed in the following text as a date in mm/dd/yyyy format. Today is September 23rd 2023.\", dateTemplate, model)\n",
        "\n",
        "#Defining Node Connections\n",
        "intencion_node.edges = [intencion_node]\n",
        "name_node.edges = [name_edge]\n",
        "dni_node.edges = [dni_edge]\n",
        "contact_node.edges = [contact_phone_edge, contact_email_edge]\n",
        "budget_node.edges = [budget_edge]\n",
        "avail_node.edges = [avail_edge]\n",
        "\n",
        "#defining edge connections\n",
        "intencion_edge.out_node = name_edge\n",
        "name_edge.out_node = dni_node\n",
        "dni_edge.out_node = contact_node\n",
        "contact_phone_edge.out_node = budget_node\n",
        "contact_email_edge.out_node = budget_node\n",
        "budget_edge.out_node = avail_node\n",
        "\n",
        "#running the graph\n",
        "current_node = name_node\n",
        "while current_node is not None:\n",
        "    res = current_node.execute()\n",
        "    print(res)\n",
        "    if res['continue']:\n",
        "        current_node = res['continue_to']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        },
        "id": "u5nSWAnarv7Z",
        "outputId": "2152c765-1210-4402-c403-b183518ec3fb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Bot =======\n",
            "Hello! My name's Dana and I'll be getting you started on your renting journey. I'll be asking you a few questions, and then forwarding you to one of our excellent agents to help you find a place you'd love to call home.\n",
            "\n",
            "First, can you please provide your name?\n",
            "======= Human Input =======\n",
            "input: helena\n",
            "*Info* parse results: {'continue': True, 'result': nameTemplate(output='helena'), 'num_fails': 0, 'continue_to': <__main__.Node object at 0x7d85419e3cd0>}\n",
            "{'continue': True, 'result': nameTemplate(output='helena'), 'num_fails': 0, 'continue_to': <__main__.Node object at 0x7d85419e3cd0>}\n",
            "======= Bot =======\n",
            "Podrias decirme tu DNI ?\n",
            "======= Human Input =======\n",
            "input: 34733833\n",
            "======= Bot =======\n",
            "Disculpa, no entiendo. Necesitamos tu numero de documento para poder continuar. Podrias facilitarlo ?\n",
            "======= Human Input =======\n",
            "input: documento 287272223\n",
            "*Info* parse results: {'continue': True, 'result': dniTemplate(output='28.727.222.3'), 'num_fails': 0, 'continue_to': <__main__.Node object at 0x7d85419e3340>}\n",
            "{'continue': True, 'result': dniTemplate(output='28.727.222.3'), 'num_fails': 0, 'continue_to': <__main__.Node object at 0x7d85419e3340>}\n",
            "======= Bot =======\n",
            "do you have a phone number or email we can use to contact you?\n",
            "======= Human Input =======\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-82716adf841d>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mcurrent_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mcurrent_node\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'continue'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-e95de3b78734>\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m#getting users input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0m_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m#running through edges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-f7f19a9a5fb4>\u001b[0m in \u001b[0;36muser_input\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \"\"\"\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'======= Human Input ======='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparsing_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}